本文主要记录如何从脉脉爬取一些会员的基本信息。

	# encoding=utf-8
	 
	import time
	import json
	import pathlib
	import requests
	import logging
	import random
	 
	# 设置日志的输出样式
	logging.basicConfig(level=logging.INFO,
	                    format='[%(asctime)-15s] [%(levelname)8s] [%(name)10s ] - %(message)s (%(filename)s:%(lineno)s)',
	                    datefmt='%Y-%m-%d %T'
	                    )
	logger = logging.getLogger(__name__)
	 
	# 需手动修改 查询关键字对应的参数
	query = "%E4%B8%8A%E6%B5%B7%20NLP"
	# 阿里算法
	# query = "%E9%98%BF%E9%87%8C%20%E7%AE%97%E6%B3%95"
	# 腾讯算法
	# query = "%E8%85%BE%E8%AE%AF%20%E7%AE%97%E6%B3%95"
	 
	# 需手动修改
	cookie = '_buuid=230106859; maimai_version=4.0.0; channel=www; sessqESmskJ1MhFFc_E'
	 
	headers = {
	    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.90 Safari/537.36',
	    'Referer': 'https://maimai.cn/web/search_center?type=contact&query={}&highlight=true'.format(query),
	    'Cookie': cookie,
	    'accept': '*/*',
	    'accept-encoding': 'gzip, deflate, br',
	    'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7'
	}
	 
	base_url = "https://maimai.cn/search/contacts?count=20&page={}&query=" + query + "&dist=0&searchTokens=&highlight=true&jsononly=1&pc=1"
	detail_url = "https://maimai.cn/contact/detail/{}"
	 
	currentPage = 0
	 
	s = requests.Session()
	has_next = True
	while has_next:
	    logger.info("开始爬取第 {} 页".format(currentPage))
	    pagePath = pathlib.Path('D:\\scrapy\\maimai\\{}.txt'.format(currentPage))
	    if pagePath.exists():
	        logger.info("使用本地文件。。。")
	        with open('D:\\scrapy\\maimai\\{}.txt'.format(currentPage), 'r', encoding='utf-8') as f:
	            json_str = f.read()
	    else:
	        logger.info("发送网络请求。。。")
	        url = base_url.format(currentPage)
	        time.sleep(5 + random.randint(0, 5))
	        try:
	            response = s.get(url, headers=headers)
	            logger.info(response.content.decode())
	        except BaseException as e:
	            logger.info(e)
	            break
	        else:
	            json_str = response.content.decode()
	            with open('D:\\scrapy\\maimai\\{}.txt'.format(currentPage), 'w', encoding='utf-8') as f:
	                f.write(response.content.decode())
	        # 测试用，只爬一页
	        # break
	    currentPage = currentPage + 1
	    if json_str:
	        # 将响应内容转换为Json对象
	        jsonobj = json.loads(json_str)
	        if jsonobj['result'] == 'ok':
	            contacts = jsonobj['data']['contacts']
	            if len(contacts) > 0:
	                for contact in contacts:
	                    uid = contact['uid']
	                    encode_mmid = contact['contact']['encode_mmid']
	                    pagePath = pathlib.Path('D:\\scrapy\\maimai\\detail{}.txt'.format(uid))
	                    if pagePath.exists():
	                        with open('D:\\scrapy\\maimai\\detail{}.txt'.format(uid), 'r', encoding='gb18030') as f:
	                            detail_str = f.read()
	                    else:
	                        durl = detail_url.format(encode_mmid)
	                        time.sleep(5 + random.randint(0, 5))
	                        try:
	                            response = s.get(durl, headers=headers)
	                            logger.info(response.content.decode())
	                        except BaseException as e:
	                            logger.info(e)
	                            has_next = False
	                            break
	                        else:
	                            detail_str = response.content.decode()
	                            with open('D:\\scrapy\\maimai\\detail{}.txt'.format(uid), 'w', encoding='gb18030') as f:
	                                f.write(response.content.decode())
	                                # break
	                    if detail_str.find("休息一会儿吧") != -1:
	                        has_next = False
	                        break
	 
	            else:
	                has_next = False
	        else:
	            has_next = False

使用以下代码将各个文件中的内容统一导出到Excel。

	import os
	import re
	import json
	import csv
	 
	re_pattern = re.compile(r'JSON.parse\(\"(.*?)\"\);</script>', re.S)
	 
	rootdir = 'D:\\scrapy\\maimai\\'
	list = os.listdir(rootdir)  # 列出文件夹下所有的目录与文件
	rows = []
	for i in range(0, len(list)):
	    row = []
	    path = os.path.join(rootdir, list[i])
	    if os.path.isfile(path):
	        with open(path, 'r', encoding='gb18030') as f:
	            detail_str = f.read()
	        if detail_str:
	            ret_str = re_pattern.search(detail_str)
	            if ret_str:
	                djson_str = ret_str.group(1).replace("\\u0022", "\"").replace("\\u002D", "-")
	                try:
	                    djson_obj = json.loads(djson_str)
	                except BaseException as e:
	                    continue
	                else:
	                    uinfo = djson_obj['data']['uinfo']
	                    card = djson_obj['data']['card']
	                    row.append(uinfo['realname'] + '(' + str(uinfo['id']) + ')')
	                    mobile = uinfo['mobile'] if uinfo['mobile'] is not None else '无'
	                    row.append(mobile)
	                    gender = '无'
	                    if uinfo['gender'] == 1:
	                        gender = '男'
	                    elif uinfo['gender'] == 2:
	                        gender = '女'
	                    row.append(gender)
	                    email = uinfo['email'] if uinfo['email'] is not None else '无'
	                    row.append(email)
	                    row.append(card['province'] + card['city'])
	                    row.append(card['company'] + '(' + str(card['cid']) + ')')
	                    row.append(card['position'])
	                    row.append(card['rank'])
	                    row.append(card['user_pfmj']['pf_name1'] + '|' + card['user_pfmj']['mj_name1'])
	                    work_exps = uinfo['work_exp']
	                    works = []
	                    for work_exp in work_exps:
	                        end_date = work_exp['end_date'] if work_exp['end_date'] is not None else '至今'
	 
	                        if work_exp['start_date'] is not None:
	                            work = work_exp['company'] + '(' + str(work_exp['start_date']) + '~' + str(
	                                end_date) + ')'
	                        else:
	                            work = work_exp['company']
	                        works.append(work)
	                    row.append(works)
	                    edu_exps = uinfo['education']
	                    edus = []
	                    for edu_exp in edu_exps:
	                        if 'end_date' in edu_exp:
	                            end_date = edu_exp['end_date'] if edu_exp['end_date'] is not None else '/'
	                        else:
	                            end_date = '/'
	                        if 'start_date' in edu_exp and edu_exp['start_date'] is not None:
	                            edu_str = str(work_exp['start_date']) + '~' + str(end_date)
	 
	                        if edu_exp['department'] is not None:
	                            if edu_str is not None:
	                                edu_str = edu_str + "," + edu_exp['department']
	                            else:
	                                edu_str = edu_exp['department']
	 
	                        if edu_exp['degree'] is not None:
	                            if edu_exp['degree'] == 3:
	                                degree = '博士'
	                            elif edu_exp['degree'] == 2:
	                                degree = '硕士'
	                            elif edu_exp['degree'] == 1:
	                                degree = '学士'
	                            else:
	                                degree = None
	                        if degree is not None:
	                            if edu_str is not None:
	                                edu_str = edu_str + "," + degree
	                            else:
	                                edu_str = degree
	 
	                        edu = edu_exp['school'] + '(' + edu_str + ')'
	                        if edu_exp['description'] is not None and edu_exp['description'] != "":
	                            edu = edu + '(' + edu_exp['description'] + ')'
	                        edus.append(edu)
	                    row.append(edus)
	                    row.append(uinfo['weibo_tags'])
	            else:
	                has_next = False
	                break
	        rows.append(row)
	 
	header = ['姓名', '手机号', '性别', '邮箱', '城市', '公司', '职位', '影响力', '职场词', '工作经历', '教育经历', '微博标签']
	 
	# 将表头数据和爬虫数据导出到Excel文件
	with open('D:\\scrapy\\maimai\\1.csv', 'w', encoding='gb18030') as f:
	    f_csv = csv.writer(f)
	    f_csv.writerow(header)
	    f_csv.writerows(rows)
	 